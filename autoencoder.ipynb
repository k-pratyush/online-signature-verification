{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from statistics import mean\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_pickle(\"training/x_train.pkl\")\n",
    "y_train = pd.read_pickle(\"training/y_train.pkl\")\n",
    "x_test = pd.read_pickle(\"training/x_test.pkl\")\n",
    "y_test = pd.read_pickle(\"training/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user', 'x_len', 'y_len', 'avg_ps', 'ratio', 'dif_pen', 'dif_az',\n",
      "       'pen_up', 'sign_time', 'x_speed', 'y_speed', 'x_size', 'total_length',\n",
      "       'tx0', 'ty0'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['user', 'x_len', 'y_len', 'avg_ps', 'ratio', 'dif_pen', 'dif_az',\n",
       "       'pen_up', 'sign_time', 'x_speed',\n",
       "       ...\n",
       "       'tx98', 'ty98', 'tp98', 'ta98', 'te98', 'tx99', 'ty99', 'tp99', 'ta99',\n",
       "       'te99'],\n",
       "      dtype='object', length=513)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.columns[0:15])\n",
    "x_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xt = torch.tensor(X_train.values)\n",
    "# yt = torch.tensor(Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TensorDataset(xt, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dl = DataLoader(dataset, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user   x_len   y_len         avg_ps     ratio  dif_pen  dif_az  pen_up  \\\n",
      "3062    31  1600.0  1135.0     440.445860  1.409692        9      20      30   \n",
      "4441    86  2818.0  1549.0     437.117845  1.819238       12      31      71   \n",
      "562     23  4675.0  4734.0     460.980707  0.987537       26      64     138   \n",
      "512     21  3531.0  1282.0     407.962500  2.754290        6      18       0   \n",
      "1311    56  2151.0   812.0  491095.406360  2.649015       14      28      94   \n",
      "40       1  2843.0  1093.0     812.722922  2.601098        7      22      72   \n",
      "3746    59  3818.0  1067.0  374332.129964  3.578257        8      22      62   \n",
      "3756    59  2531.0  1185.0     415.081481  2.135865        7      18      44   \n",
      "44       1  2656.0  1557.0     847.739938  1.705845       18      30      43   \n",
      "575     24  3819.0  3239.0     261.134367  1.179068       36      44     448   \n",
      "2274     0  3612.0  1598.0     681.326733  2.260325       15      19       0   \n",
      "2235    98  2272.0  1074.0     500.086758  2.115456       11      25      70   \n",
      "591     25  2974.0  1794.0     442.245968  1.657748       30      56     390   \n",
      "4582    92  2242.0  1685.0      59.228532  1.330564       17      28     361   \n",
      "67       2  3116.0  1513.0  454304.897314  2.059484       12      27     163   \n",
      "2222    97  3277.0  2051.0  642373.848987  1.597757       13      31      91   \n",
      "265     11  3765.0  1381.0     195.840961  2.726285       21      35     620   \n",
      "2170    94  4828.0  3021.0     356.082789  1.598146       14      49     250   \n",
      "127      5  3270.0  1322.0     687.739130  2.473525       21      25      53   \n",
      "1862    80  3459.0  1127.0     241.042254  3.069210       23      22     167   \n",
      "226      9  4453.0  1745.0     943.779904  2.551862       16      20       0   \n",
      "4767    99  1400.0  2152.0     423.244186  0.650558       11      48      69   \n",
      "723     30  2538.0  1810.0     302.000000  1.402210        7      19     174   \n",
      "2750    19  2522.0  1094.0     262.997585  2.305302       11      27      81   \n",
      "4564    91  2941.0   981.0     423.762025  2.997961       12      25      48   \n",
      "2454     7  2513.0   640.0     355.060241  3.926563        7      23      36   \n",
      "1474    63  2898.0   840.0     204.668224  3.450000       22      31     337   \n",
      "3517    49  2727.0  1166.0     509.658974  2.338765        7      28      81   \n",
      "1192    51  2998.0  1577.0     480.232843  1.901078       14      32     107   \n",
      "1902    82  2075.0  1226.0     574.742424  1.692496        8      17       0   \n",
      "...    ...     ...     ...            ...       ...      ...     ...     ...   \n",
      "2677    16  3466.0  1662.0     790.392157  2.085439       13      22       0   \n",
      "4306    81  3645.0  1373.0     486.840630  2.654771       11      23      91   \n",
      "4335    82  1928.0  1171.0     544.055556  1.646456        7      13       0   \n",
      "2422     6  3123.0  1198.0     620.692082  2.606845        8      27      12   \n",
      "2297     1  2359.0  1270.0     924.290323  1.857480        6      25      14   \n",
      "2865    23  2690.0  1920.0     532.699029  1.401042       11      13      41   \n",
      "881     37  2611.0  1937.0     296.207090  1.347961       14      22     246   \n",
      "3927    66  2633.0  1240.0     554.298361  2.123387        7      21      43   \n",
      "4405    85  2228.0   911.0     515.016393  2.445664        6      18      52   \n",
      "970     40  2842.0  1361.0     310.663685  2.088170       20      31     191   \n",
      "1358    58  3904.0  2670.0  834247.863248  1.462172       10      24       7   \n",
      "4707    97  2867.0  1452.0     473.002967  1.974518       14      35     196   \n",
      "4331    82  2448.0  1394.0     489.636364  1.756098        8      11       0   \n",
      "1411    61  3043.0  1148.0     243.176938  2.650697       13      18     183   \n",
      "3601    53  3377.0  2188.0     486.840376  1.543419       21      39     166   \n",
      "1262    54  1948.0  1114.0     450.944712  1.748654       10      20      61   \n",
      "4200    77  3009.0  1658.0     414.333973  1.814837       12      34     167   \n",
      "4545    91  3254.0   982.0     405.478481  3.313646        9      20      46   \n",
      "4165    75  3538.0  1122.0     208.374220  3.153298       18      27      63   \n",
      "4127    74  3052.0  1278.0     269.649180  2.388106       11      31      77   \n",
      "2007    87  3039.0  1070.0     270.493554  2.840187       11      32     200   \n",
      "2871    23  2490.0  1397.0     502.338798  1.782391        9      21      39   \n",
      "4558    91  3015.0  1177.0  487947.368421  2.561597       30      30      64   \n",
      "3671    56  2940.0  1209.0     738.744409  2.431762        8      27      53   \n",
      "2055    89  2743.0  1266.0     223.956931  2.166667       12      25     275   \n",
      "1364    59  1317.0   963.0     414.299169  1.367601        9      15     168   \n",
      "4283    80  3035.0  1102.0     151.315603  2.754083       22      22      89   \n",
      "3645    55  4206.0  1169.0     434.854725  3.597947        9      22     162   \n",
      "4702    97  3275.0  1406.0     472.134128  2.329303       17      32     192   \n",
      "4334    82  1678.0  1007.0     527.480000  1.666336        3      12       0   \n",
      "\n",
      "      sign_time    x_speed  ...        tx98       ty98        tp98    ta98  \\\n",
      "3062       1.57  10.191083  ...   90.915034  14.626831      719.04  155.00   \n",
      "4441       2.97   9.488215  ...   16.691778  45.659548      818.32  175.12   \n",
      "562        3.11  15.032154  ...   56.723693  28.606576     1023.00  122.00   \n",
      "512        0.80  44.137500  ...   95.609117  64.523140      441.80  157.40   \n",
      "1311       2.83   7.600707  ...   87.087063  69.395004  1023000.00  115.00   \n",
      "40         3.97   7.161209  ...   65.543235  57.717659     1023.00  163.12   \n",
      "3746       2.77  13.783394  ...   92.437258  65.149376  1023000.00  121.54   \n",
      "3756       2.70   9.374074  ...   89.252033  54.635221     1001.80  136.00   \n",
      "44         3.23   8.222910  ...   76.199339  33.572566     1023.00  142.46   \n",
      "575        7.74   4.934109  ...   70.258367  25.149450      759.00  104.00   \n",
      "2274       1.01  35.762376  ...    3.056793   3.799235      698.64  135.00   \n",
      "2235       4.38   5.187215  ...    9.217035  27.166376      508.08  132.00   \n",
      "591        9.92   2.997984  ...   72.023086  41.766145        0.00  128.68   \n",
      "4582       7.22   3.105263  ...   93.039455  79.476486        0.00  133.00   \n",
      "67         6.33   4.922591  ...   90.075565  36.658677  1023000.00  147.68   \n",
      "2222       5.43   6.034991  ...   94.546073  51.237563  1023000.00  144.56   \n",
      "265        8.74   4.307780  ...   62.922821   9.601224      804.16  287.48   \n",
      "2170       9.18   5.259259  ...   72.506118  27.477208        0.00  294.28   \n",
      "127        4.14   7.898551  ...   97.583847  33.942162     1023.00  159.00   \n",
      "1862       4.97   6.959759  ...   88.169651  16.067838      748.12  187.94   \n",
      "226        2.09  21.306220  ...   91.450051  25.156182     1023.00   90.00   \n",
      "4767       2.58   5.426357  ...   87.913635  41.355909      755.64  304.48   \n",
      "723        4.80   5.287500  ...    7.618491   5.397438      710.20  140.00   \n",
      "2750       4.14   6.091787  ...   87.425828  48.446069      565.44  107.56   \n",
      "4564       3.95   7.445570  ...  100.000000  97.649191      973.50  162.00   \n",
      "2454       1.66  15.138554  ...   99.518479  44.746016      505.16  172.64   \n",
      "1474       6.42   4.514019  ...   81.013366  72.615508        0.00  343.00   \n",
      "3517       3.90   6.992308  ...   27.528380  83.220961      452.00  132.00   \n",
      "1192       4.08   7.348039  ...   89.254467  35.654114      734.28  123.68   \n",
      "1902       0.66  31.439394  ...   96.091205  42.055294      561.88  130.00   \n",
      "...         ...        ...  ...         ...        ...         ...     ...   \n",
      "2677       0.51  67.960784  ...    2.792601  25.682591      715.56  156.04   \n",
      "4306       5.71   6.383538  ...   88.292012   3.579820      814.02  182.42   \n",
      "4335       0.54  35.703704  ...   86.110023  20.319647      658.28  161.00   \n",
      "2422       3.41   9.158358  ...   55.752691   6.176729      809.66  175.00   \n",
      "2297       2.17  10.870968  ...   92.364843  16.906109     1001.48  121.00   \n",
      "2865       2.06  13.058252  ...   91.706296  74.617300      991.08  110.00   \n",
      "881        5.36   4.871269  ...   90.063918  22.167099      703.60  134.44   \n",
      "3927       3.05   8.632787  ...   35.425811  17.081294      685.50  123.00   \n",
      "4405       2.44   9.131148  ...   91.243477  20.121218     1023.00  152.24   \n",
      "970        5.59   5.084079  ...   48.937906   1.979938      820.76  137.82   \n",
      "1358       2.34  16.683761  ...   97.949329  40.068894  1023000.00  134.28   \n",
      "4707       6.74   4.253709  ...   76.066176  20.883734      242.84  195.00   \n",
      "4331       0.55  44.509091  ...   95.589507  42.130066      338.20  171.90   \n",
      "1411       5.03   6.049702  ...   73.593199  59.925448      335.82  142.00   \n",
      "3601       6.39   5.284820  ...   98.353780  41.525847      850.56  130.44   \n",
      "1262       4.16   4.682692  ...   63.134249  12.310679      737.00  150.00   \n",
      "4200       5.21   5.775432  ...   85.422124  95.526450      857.12  127.00   \n",
      "4545       3.95   8.237975  ...   86.530013  92.707544       42.70  158.10   \n",
      "4165       4.81   7.355509  ...   76.346785  24.274815      403.84  138.00   \n",
      "4127       3.05  10.006557  ...   92.088631  50.871018      403.30  151.00   \n",
      "2007       5.43   5.596685  ...   96.489848  39.312975      290.68  137.42   \n",
      "2871       1.83  13.606557  ...   90.299712  46.104837      936.86  123.00   \n",
      "4558       4.56   6.611842  ...   94.531783  83.942226   910280.00  152.00   \n",
      "3671       3.13   9.392971  ...   97.491007  38.731571      916.52  141.00   \n",
      "2055       7.43   3.691790  ...   90.050933  62.317808        0.00  144.00   \n",
      "1364       3.61   3.648199  ...   82.189673  15.784008     1023.00  134.56   \n",
      "4283       2.82  10.762411  ...   41.023683  16.876897      321.44  162.00   \n",
      "3645       7.09   5.932299  ...   65.614598  19.059025      499.20  139.00   \n",
      "4702       6.71   4.880775  ...   70.570019  11.525038        0.00  188.42   \n",
      "4334       0.50  33.560000  ...   78.375612  41.927702      636.00  160.00   \n",
      "\n",
      "       te98        tx99        ty99        tp99    ta99   te99  \n",
      "3062  65.00  100.000000   17.004405      428.88  155.00  65.00  \n",
      "4441  57.94    7.345635   39.122014      869.97  176.97  56.00  \n",
      "562   61.00   65.561497   30.038023      958.26  125.67  62.00  \n",
      "512   55.60  100.000000   65.912637      254.00  158.00  55.00  \n",
      "1311  60.00  100.000000   80.172414   937230.00  116.00  60.00  \n",
      "40    59.00   75.026381   60.384263     1023.00  167.00  58.00  \n",
      "3746  59.54  100.000000   71.040300   947850.00  121.00  59.00  \n",
      "3756  61.00   96.760174   57.974684      897.70  134.00  61.00  \n",
      "44    59.00   85.881024   30.186256     1023.00  141.23  59.77  \n",
      "575   66.00   70.411102   20.994134      759.98  103.26  65.26  \n",
      "2274  67.00    4.401993    1.001252      183.18  135.00  67.00  \n",
      "2235  64.00   19.894366   26.350093      621.24  130.76  63.38  \n",
      "591   63.00   91.324815   52.396878      655.00  120.32  64.00  \n",
      "4582  50.00   90.544157   61.721068      149.48  133.00  50.00  \n",
      "67    58.66   99.679076   37.210839  1023000.00  155.00  55.00  \n",
      "2222  60.00   86.298444   50.316919  1023000.00  158.57  59.43  \n",
      "265   56.00   74.077025    0.144823      937.38  284.00  59.00  \n",
      "2170  63.00   72.452361   17.014234        0.00  290.00  63.00  \n",
      "127   61.00   93.730887   28.366112      998.06  158.00  62.00  \n",
      "1862  50.00  100.000000   22.182786      721.58  180.00  54.03  \n",
      "226   71.82  100.000000   25.386819      972.95   90.00  74.82  \n",
      "4767  74.00   81.071429   52.323420      490.74  300.74  74.00  \n",
      "723   50.00    0.000000    0.000000      701.20  136.80  48.00  \n",
      "2750  59.00   99.206979   46.709324      574.84  105.00  59.00  \n",
      "4564  66.00   93.233594   98.063201     1022.70  160.95  67.05  \n",
      "2454  58.68  100.000000   36.718750      454.90  172.00  59.00  \n",
      "1474  59.00   81.504486   77.261905        0.00  345.00  59.00  \n",
      "3517  50.00   36.230290   89.022298      503.70  132.00  51.00  \n",
      "1192  67.00   99.466311   37.539632      667.48  124.16  66.08  \n",
      "1902  65.00  100.000000   44.535073      519.00  130.00  65.00  \n",
      "...     ...         ...         ...         ...     ...    ...  \n",
      "2677  55.00    0.000000   23.225030      711.00  156.00  55.00  \n",
      "4306  57.00   97.146776    0.000000      864.74  182.00  59.00  \n",
      "4335  57.00   90.715768   22.715628      647.00  161.00  57.00  \n",
      "2422  57.00   57.028498    1.419032      677.31  177.00  57.00  \n",
      "2297  69.00  100.000000   20.078740      968.65  122.00  68.00  \n",
      "2865  52.00  100.000000   81.510417      796.78  111.00  53.00  \n",
      "881   62.00   99.885101   28.910687      814.88  130.72  63.00  \n",
      "3927  59.00   44.322066   22.258065      685.60  123.00  59.00  \n",
      "4405  60.00  100.000000   14.818880     1015.16  154.56  61.12  \n",
      "970   63.82   59.711471    6.171932      762.91  137.59  64.41  \n",
      "1358  62.00   94.620902   38.239700   858320.00  138.66  61.00  \n",
      "4707  51.00   79.734915   12.603306     1023.00  196.00  52.26  \n",
      "4331  57.00  100.000000   43.974175      304.00  172.00  57.00  \n",
      "1411  61.00   70.161025   47.038328      341.06  141.00  61.00  \n",
      "3601  64.78   99.170862   44.424132      848.00  134.22  63.00  \n",
      "1262  64.00   72.433265   14.991023      748.32  145.16  64.84  \n",
      "4200  62.00   99.102692  100.000000      902.79  129.00  63.00  \n",
      "4545  56.10   90.749846   98.778004      888.70  161.95  58.05  \n",
      "4165  48.00   87.196156   28.431373      362.26  141.19  50.38  \n",
      "4127  50.00  100.000000   59.546166      447.05  151.95  50.95  \n",
      "2007  67.00  100.000000   33.177570      525.27  143.71  68.00  \n",
      "2871  53.68  100.000000   53.256979      813.93  123.00  55.00  \n",
      "4558  36.88   93.598673   88.445200  1010120.00  150.56  39.44  \n",
      "3671  59.74  100.000000   29.445823     1023.00  135.26  62.87  \n",
      "2055  66.00   91.323369   60.189573      674.32  145.00  65.00  \n",
      "1364  62.78   92.255125   20.872274      982.09  136.22  63.39  \n",
      "4283  54.00   51.202636   15.698730      332.66  163.00  54.00  \n",
      "3645  58.00   66.000951   14.798973      796.48  140.00  58.00  \n",
      "4702  70.42   76.977099    5.618777     1023.00  188.87  70.00  \n",
      "4334  60.00   83.134684   44.786495      636.00  160.00  60.00  \n",
      "\n",
      "[3337 rows x 513 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(input_shape = (513,),activation = 'relu',units = 513),\n",
    "    tf.keras.layers.Dense(400, activation='relu'),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(200,activation='relu'),\n",
    "    tf.keras.layers.Dense(300,activation='relu'),\n",
    "    tf.keras.layers.Dense(400,activation= 'relu'),\n",
    "    tf.keras.layers.Dense(513,activation = 'relu')\n",
    "  ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
    "                metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 513)               263682    \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 400)               205600    \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 400)               120400    \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 513)               205713    \n",
      "=================================================================\n",
      "Total params: 1,086,595\n",
      "Trainable params: 1,086,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3337 samples, validate on 249 samples\n",
      "Epoch 1/120\n",
      "3337/3337 [==============================] - 3s 780us/sample - loss: 6.4223 - accuracy: 0.0192 - val_loss: 5.5230 - val_accuracy: 0.0161\n",
      "Epoch 2/120\n",
      "3337/3337 [==============================] - 1s 432us/sample - loss: 5.3234 - accuracy: 0.0066 - val_loss: 5.0329 - val_accuracy: 0.0201\n",
      "Epoch 3/120\n",
      "3337/3337 [==============================] - 1s 378us/sample - loss: 5.0354 - accuracy: 0.0195 - val_loss: 4.8098 - val_accuracy: 0.0241\n",
      "Epoch 4/120\n",
      "3337/3337 [==============================] - 1s 393us/sample - loss: 4.9027 - accuracy: 0.0306 - val_loss: 4.7396 - val_accuracy: 0.0843\n",
      "Epoch 5/120\n",
      "3337/3337 [==============================] - 1s 393us/sample - loss: 4.7246 - accuracy: 0.1378 - val_loss: 4.6093 - val_accuracy: 0.1406\n",
      "Epoch 6/120\n",
      "3337/3337 [==============================] - 1s 383us/sample - loss: 4.7376 - accuracy: 0.4699 - val_loss: 4.6391 - val_accuracy: 0.1526\n",
      "Epoch 7/120\n",
      "3337/3337 [==============================] - 1s 392us/sample - loss: 4.6384 - accuracy: 0.2164 - val_loss: 4.4845 - val_accuracy: 0.1365\n",
      "Epoch 8/120\n",
      "3337/3337 [==============================] - 1s 404us/sample - loss: 4.6715 - accuracy: 0.6662 - val_loss: 4.4255 - val_accuracy: 0.8916\n",
      "Epoch 9/120\n",
      "3337/3337 [==============================] - 1s 416us/sample - loss: 4.4817 - accuracy: 0.7899 - val_loss: 4.3076 - val_accuracy: 0.7510\n",
      "Epoch 10/120\n",
      "3337/3337 [==============================] - 1s 389us/sample - loss: 4.4884 - accuracy: 0.9038 - val_loss: 4.2689 - val_accuracy: 0.9438\n",
      "Epoch 11/120\n",
      "3337/3337 [==============================] - 1s 378us/sample - loss: 4.2768 - accuracy: 0.9305 - val_loss: 4.2212 - val_accuracy: 0.9438\n",
      "Epoch 12/120\n",
      "3337/3337 [==============================] - 2s 474us/sample - loss: 4.2115 - accuracy: 0.9275 - val_loss: 4.0358 - val_accuracy: 0.9438\n",
      "Epoch 13/120\n",
      "3337/3337 [==============================] - 2s 493us/sample - loss: 4.1721 - accuracy: 0.9311 - val_loss: 4.0822 - val_accuracy: 0.9438\n",
      "Epoch 14/120\n",
      "3337/3337 [==============================] - 1s 438us/sample - loss: 4.1527 - accuracy: 0.9311 - val_loss: 3.9776 - val_accuracy: 0.9438\n",
      "Epoch 15/120\n",
      "3337/3337 [==============================] - 1s 388us/sample - loss: 4.1018 - accuracy: 0.9311 - val_loss: 3.9217 - val_accuracy: 0.9438\n",
      "Epoch 16/120\n",
      "3337/3337 [==============================] - 1s 376us/sample - loss: 3.9969 - accuracy: 0.9281 - val_loss: 3.8247 - val_accuracy: 0.9438\n",
      "Epoch 17/120\n",
      "3337/3337 [==============================] - 1s 393us/sample - loss: 3.9731 - accuracy: 0.9281 - val_loss: 3.6935 - val_accuracy: 0.9438\n",
      "Epoch 18/120\n",
      "3337/3337 [==============================] - 1s 410us/sample - loss: 3.7446 - accuracy: 0.9308 - val_loss: 3.5611 - val_accuracy: 0.9438\n",
      "Epoch 19/120\n",
      "3337/3337 [==============================] - 2s 516us/sample - loss: 3.6558 - accuracy: 0.9305 - val_loss: 3.4630 - val_accuracy: 0.9398\n",
      "Epoch 20/120\n",
      "3337/3337 [==============================] - 2s 633us/sample - loss: 3.6467 - accuracy: 0.9305 - val_loss: 3.5228 - val_accuracy: 0.9438\n",
      "Epoch 21/120\n",
      "3337/3337 [==============================] - 2s 480us/sample - loss: 3.6038 - accuracy: 0.9311 - val_loss: 3.4992 - val_accuracy: 0.9438\n",
      "Epoch 22/120\n",
      "3337/3337 [==============================] - 2s 491us/sample - loss: 3.5422 - accuracy: 0.9299 - val_loss: 3.4158 - val_accuracy: 0.9438\n",
      "Epoch 23/120\n",
      "3337/3337 [==============================] - 2s 464us/sample - loss: 3.5201 - accuracy: 0.9302 - val_loss: 3.3774 - val_accuracy: 0.9438\n",
      "Epoch 24/120\n",
      "3337/3337 [==============================] - 2s 552us/sample - loss: 3.4380 - accuracy: 0.9296 - val_loss: 3.2933 - val_accuracy: 0.9438\n",
      "Epoch 25/120\n",
      "3337/3337 [==============================] - 2s 608us/sample - loss: 3.4026 - accuracy: 0.9311 - val_loss: 3.2768 - val_accuracy: 0.9438\n",
      "Epoch 26/120\n",
      "3337/3337 [==============================] - 1s 449us/sample - loss: 3.4067 - accuracy: 0.9311 - val_loss: 3.2539 - val_accuracy: 0.9438\n",
      "Epoch 27/120\n",
      "3337/3337 [==============================] - 2s 482us/sample - loss: 3.5044 - accuracy: 0.9287 - val_loss: 3.6221 - val_accuracy: 0.9438\n",
      "Epoch 28/120\n",
      "3337/3337 [==============================] - 2s 566us/sample - loss: 3.5293 - accuracy: 0.9311 - val_loss: 3.2812 - val_accuracy: 0.9438\n",
      "Epoch 29/120\n",
      "3337/3337 [==============================] - 2s 517us/sample - loss: 3.3550 - accuracy: 0.9290 - val_loss: 3.2688 - val_accuracy: 0.9438\n",
      "Epoch 30/120\n",
      "3337/3337 [==============================] - 2s 588us/sample - loss: 3.2613 - accuracy: 0.9299 - val_loss: 3.3695 - val_accuracy: 0.9438\n",
      "Epoch 31/120\n",
      "3337/3337 [==============================] - 2s 555us/sample - loss: 3.3882 - accuracy: 0.9308 - val_loss: 3.1742 - val_accuracy: 0.9438\n",
      "Epoch 32/120\n",
      "3337/3337 [==============================] - 2s 465us/sample - loss: 3.3325 - accuracy: 0.9311 - val_loss: 3.1760 - val_accuracy: 0.9438\n",
      "Epoch 33/120\n",
      "3337/3337 [==============================] - 2s 510us/sample - loss: 3.2408 - accuracy: 0.9305 - val_loss: 3.1250 - val_accuracy: 0.9438\n",
      "Epoch 34/120\n",
      "3337/3337 [==============================] - 2s 535us/sample - loss: 3.2170 - accuracy: 0.9308 - val_loss: 2.9816 - val_accuracy: 0.9398\n",
      "Epoch 35/120\n",
      "3337/3337 [==============================] - 2s 482us/sample - loss: 3.1122 - accuracy: 0.9290 - val_loss: 3.0232 - val_accuracy: 0.9438\n",
      "Epoch 36/120\n",
      "3337/3337 [==============================] - 1s 447us/sample - loss: 3.0662 - accuracy: 0.9305 - val_loss: 2.9256 - val_accuracy: 0.9398\n",
      "Epoch 37/120\n",
      "3337/3337 [==============================] - 2s 618us/sample - loss: 2.9970 - accuracy: 0.9284 - val_loss: 2.9186 - val_accuracy: 0.9438\n",
      "Epoch 38/120\n",
      "3337/3337 [==============================] - 2s 472us/sample - loss: 2.9726 - accuracy: 0.9296 - val_loss: 2.8697 - val_accuracy: 0.9398\n",
      "Epoch 39/120\n",
      "3337/3337 [==============================] - 2s 475us/sample - loss: 2.9943 - accuracy: 0.9311 - val_loss: 2.8669 - val_accuracy: 0.9438\n",
      "Epoch 40/120\n",
      "3337/3337 [==============================] - 1s 399us/sample - loss: 3.0194 - accuracy: 0.9302 - val_loss: 2.8697 - val_accuracy: 0.9398\n",
      "Epoch 41/120\n",
      "3337/3337 [==============================] - 1s 396us/sample - loss: 2.9733 - accuracy: 0.9305 - val_loss: 2.7830 - val_accuracy: 0.9438\n",
      "Epoch 42/120\n",
      "3337/3337 [==============================] - 1s 408us/sample - loss: 2.9033 - accuracy: 0.9311 - val_loss: 2.8027 - val_accuracy: 0.9438\n",
      "Epoch 43/120\n",
      "3337/3337 [==============================] - 1s 419us/sample - loss: 2.8497 - accuracy: 0.9314 - val_loss: 2.7255 - val_accuracy: 0.9438\n",
      "Epoch 44/120\n",
      "3337/3337 [==============================] - 1s 392us/sample - loss: 2.8483 - accuracy: 0.9299 - val_loss: 3.3881 - val_accuracy: 0.9438\n",
      "Epoch 45/120\n",
      "3337/3337 [==============================] - 1s 412us/sample - loss: 2.9739 - accuracy: 0.9296 - val_loss: 2.7337 - val_accuracy: 0.9438\n",
      "Epoch 46/120\n",
      "3337/3337 [==============================] - 1s 434us/sample - loss: 2.7836 - accuracy: 0.9308 - val_loss: 2.7069 - val_accuracy: 0.9438\n",
      "Epoch 47/120\n",
      "3337/3337 [==============================] - 1s 418us/sample - loss: 2.8155 - accuracy: 0.9305 - val_loss: 2.6832 - val_accuracy: 0.9438\n",
      "Epoch 48/120\n",
      "3337/3337 [==============================] - 1s 406us/sample - loss: 2.7803 - accuracy: 0.9308 - val_loss: 2.6695 - val_accuracy: 0.9398\n",
      "Epoch 49/120\n",
      "3337/3337 [==============================] - 1s 376us/sample - loss: 2.7719 - accuracy: 0.9290 - val_loss: 2.6619 - val_accuracy: 0.9398\n",
      "Epoch 50/120\n",
      "3337/3337 [==============================] - 1s 388us/sample - loss: 2.7375 - accuracy: 0.9305 - val_loss: 2.6393 - val_accuracy: 0.9357\n",
      "Epoch 51/120\n",
      "3337/3337 [==============================] - 1s 403us/sample - loss: 2.7170 - accuracy: 0.9296 - val_loss: 2.6565 - val_accuracy: 0.9438\n",
      "Epoch 52/120\n",
      "3337/3337 [==============================] - 1s 406us/sample - loss: 2.7121 - accuracy: 0.9308 - val_loss: 2.5978 - val_accuracy: 0.9438\n",
      "Epoch 53/120\n",
      "3337/3337 [==============================] - 1s 399us/sample - loss: 2.7299 - accuracy: 0.9296 - val_loss: 2.6373 - val_accuracy: 0.9438\n",
      "Epoch 54/120\n",
      "3337/3337 [==============================] - 1s 398us/sample - loss: 2.7009 - accuracy: 0.9305 - val_loss: 2.6225 - val_accuracy: 0.9438\n",
      "Epoch 55/120\n",
      "3337/3337 [==============================] - 1s 405us/sample - loss: 2.6864 - accuracy: 0.9290 - val_loss: 2.5237 - val_accuracy: 0.9438\n",
      "Epoch 56/120\n",
      "3337/3337 [==============================] - 1s 399us/sample - loss: 2.6507 - accuracy: 0.9299 - val_loss: 2.5851 - val_accuracy: 0.9438\n",
      "Epoch 57/120\n",
      "3337/3337 [==============================] - 1s 386us/sample - loss: 2.6366 - accuracy: 0.9293 - val_loss: 2.5443 - val_accuracy: 0.9398\n",
      "Epoch 58/120\n",
      "3337/3337 [==============================] - 1s 388us/sample - loss: 2.6426 - accuracy: 0.9311 - val_loss: 2.5266 - val_accuracy: 0.9398\n",
      "Epoch 59/120\n",
      "3337/3337 [==============================] - 1s 406us/sample - loss: 2.6185 - accuracy: 0.9311 - val_loss: 2.5402 - val_accuracy: 0.9438\n",
      "Epoch 60/120\n",
      "3337/3337 [==============================] - 1s 406us/sample - loss: 2.6029 - accuracy: 0.9287 - val_loss: 2.5128 - val_accuracy: 0.9357\n",
      "Epoch 61/120\n",
      "3337/3337 [==============================] - 1s 397us/sample - loss: 2.5992 - accuracy: 0.9299 - val_loss: 2.4945 - val_accuracy: 0.9438\n",
      "Epoch 62/120\n",
      "3337/3337 [==============================] - 1s 423us/sample - loss: 2.6199 - accuracy: 0.9305 - val_loss: 2.5055 - val_accuracy: 0.9357\n",
      "Epoch 63/120\n",
      "3337/3337 [==============================] - 1s 434us/sample - loss: 2.5999 - accuracy: 0.9281 - val_loss: 2.4995 - val_accuracy: 0.9438\n",
      "Epoch 64/120\n",
      "3337/3337 [==============================] - 1s 414us/sample - loss: 2.5675 - accuracy: 0.9311 - val_loss: 2.4416 - val_accuracy: 0.9398\n",
      "Epoch 65/120\n",
      "3337/3337 [==============================] - 1s 433us/sample - loss: 2.5854 - accuracy: 0.9308 - val_loss: 2.4835 - val_accuracy: 0.9398\n",
      "Epoch 66/120\n",
      "3337/3337 [==============================] - 2s 653us/sample - loss: 2.5690 - accuracy: 0.9305 - val_loss: 2.4606 - val_accuracy: 0.9438\n",
      "Epoch 67/120\n",
      "3337/3337 [==============================] - 1s 424us/sample - loss: 2.5693 - accuracy: 0.9284 - val_loss: 2.4567 - val_accuracy: 0.9438\n",
      "Epoch 68/120\n",
      "3337/3337 [==============================] - 2s 506us/sample - loss: 2.5780 - accuracy: 0.9299 - val_loss: 2.4571 - val_accuracy: 0.9438\n",
      "Epoch 69/120\n",
      "3337/3337 [==============================] - 2s 473us/sample - loss: 2.5655 - accuracy: 0.9305 - val_loss: 2.4560 - val_accuracy: 0.9438\n",
      "Epoch 70/120\n",
      "3337/3337 [==============================] - 2s 544us/sample - loss: 2.5632 - accuracy: 0.9302 - val_loss: 2.4925 - val_accuracy: 0.9438\n",
      "Epoch 71/120\n",
      "3337/3337 [==============================] - 1s 388us/sample - loss: 2.5449 - accuracy: 0.9293 - val_loss: 2.4560 - val_accuracy: 0.9438\n",
      "Epoch 72/120\n",
      "3337/3337 [==============================] - 1s 377us/sample - loss: 2.5576 - accuracy: 0.9308 - val_loss: 2.4731 - val_accuracy: 0.9398\n",
      "Epoch 73/120\n",
      "3337/3337 [==============================] - 2s 554us/sample - loss: 2.5554 - accuracy: 0.9311 - val_loss: 2.4602 - val_accuracy: 0.9398\n",
      "Epoch 74/120\n",
      "3337/3337 [==============================] - 1s 410us/sample - loss: 2.5568 - accuracy: 0.9311 - val_loss: 2.4359 - val_accuracy: 0.9398\n",
      "Epoch 75/120\n",
      "3337/3337 [==============================] - 1s 404us/sample - loss: 2.5540 - accuracy: 0.9308 - val_loss: 2.4684 - val_accuracy: 0.9438\n",
      "Epoch 76/120\n",
      "3337/3337 [==============================] - 1s 410us/sample - loss: 2.5432 - accuracy: 0.9308 - val_loss: 2.4991 - val_accuracy: 0.9398\n",
      "Epoch 77/120\n",
      "3337/3337 [==============================] - 1s 414us/sample - loss: 2.5241 - accuracy: 0.9305 - val_loss: 2.4425 - val_accuracy: 0.9398\n",
      "Epoch 78/120\n",
      "3337/3337 [==============================] - 1s 418us/sample - loss: 2.4876 - accuracy: 0.9311 - val_loss: 2.3604 - val_accuracy: 0.9438\n",
      "Epoch 79/120\n",
      "3337/3337 [==============================] - 1s 423us/sample - loss: 2.4491 - accuracy: 0.9308 - val_loss: 2.3607 - val_accuracy: 0.9438\n",
      "Epoch 80/120\n",
      "3337/3337 [==============================] - 1s 400us/sample - loss: 2.4464 - accuracy: 0.9311 - val_loss: 2.4154 - val_accuracy: 0.9438\n",
      "Epoch 81/120\n",
      "3337/3337 [==============================] - 2s 697us/sample - loss: 2.5493 - accuracy: 0.9311 - val_loss: 2.4163 - val_accuracy: 0.9438\n",
      "Epoch 82/120\n",
      "3337/3337 [==============================] - 2s 542us/sample - loss: 2.4181 - accuracy: 0.9311 - val_loss: 2.3439 - val_accuracy: 0.9438\n",
      "Epoch 83/120\n",
      "3337/3337 [==============================] - 2s 494us/sample - loss: 2.3993 - accuracy: 0.9314 - val_loss: 2.2960 - val_accuracy: 0.9398\n",
      "Epoch 84/120\n",
      "3337/3337 [==============================] - 1s 441us/sample - loss: 2.4128 - accuracy: 0.9314 - val_loss: 2.3598 - val_accuracy: 0.9438\n",
      "Epoch 85/120\n",
      "3337/3337 [==============================] - 2s 461us/sample - loss: 2.5041 - accuracy: 0.9311 - val_loss: 2.6435 - val_accuracy: 0.9438\n",
      "Epoch 86/120\n",
      "3337/3337 [==============================] - 1s 432us/sample - loss: 2.4547 - accuracy: 0.9311 - val_loss: 2.4209 - val_accuracy: 0.9398\n",
      "Epoch 87/120\n",
      "3337/3337 [==============================] - 1s 422us/sample - loss: 2.4891 - accuracy: 0.9281 - val_loss: 2.2867 - val_accuracy: 0.9398\n",
      "Epoch 88/120\n",
      "3337/3337 [==============================] - 1s 412us/sample - loss: 2.4053 - accuracy: 0.9299 - val_loss: 2.3194 - val_accuracy: 0.9357\n",
      "Epoch 89/120\n",
      "3337/3337 [==============================] - 1s 395us/sample - loss: 2.3913 - accuracy: 0.9305 - val_loss: 2.3474 - val_accuracy: 0.9438\n",
      "Epoch 90/120\n",
      "3337/3337 [==============================] - 1s 430us/sample - loss: 2.3855 - accuracy: 0.9278 - val_loss: 2.2974 - val_accuracy: 0.9438\n",
      "Epoch 91/120\n",
      "3337/3337 [==============================] - 1s 436us/sample - loss: 2.3604 - accuracy: 0.9308 - val_loss: 2.2953 - val_accuracy: 0.9438\n",
      "Epoch 92/120\n",
      "3337/3337 [==============================] - 2s 518us/sample - loss: 2.4040 - accuracy: 0.9299 - val_loss: 2.2900 - val_accuracy: 0.9438\n",
      "Epoch 93/120\n",
      "3337/3337 [==============================] - 1s 445us/sample - loss: 2.3185 - accuracy: 0.9299 - val_loss: 2.2145 - val_accuracy: 0.9438\n",
      "Epoch 94/120\n",
      "3337/3337 [==============================] - 1s 440us/sample - loss: 2.2924 - accuracy: 0.9293 - val_loss: 2.2411 - val_accuracy: 0.9357\n",
      "Epoch 95/120\n",
      "3337/3337 [==============================] - 2s 570us/sample - loss: 2.2649 - accuracy: 0.9287 - val_loss: 2.1876 - val_accuracy: 0.9398\n",
      "Epoch 96/120\n",
      "3337/3337 [==============================] - 2s 493us/sample - loss: 2.2603 - accuracy: 0.9293 - val_loss: 2.1881 - val_accuracy: 0.9438\n",
      "Epoch 97/120\n",
      "3337/3337 [==============================] - 2s 541us/sample - loss: 2.2648 - accuracy: 0.9308 - val_loss: 2.1660 - val_accuracy: 0.9398\n",
      "Epoch 98/120\n",
      "3337/3337 [==============================] - 2s 467us/sample - loss: 2.2862 - accuracy: 0.9314 - val_loss: 2.1818 - val_accuracy: 0.9438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/120\n",
      "3337/3337 [==============================] - 1s 429us/sample - loss: 2.2642 - accuracy: 0.9308 - val_loss: 2.1866 - val_accuracy: 0.9438\n",
      "Epoch 100/120\n",
      "3337/3337 [==============================] - 1s 427us/sample - loss: 2.3869 - accuracy: 0.9311 - val_loss: 2.2871 - val_accuracy: 0.9438\n",
      "Epoch 101/120\n",
      "3337/3337 [==============================] - 2s 482us/sample - loss: 2.2984 - accuracy: 0.9311 - val_loss: 2.1966 - val_accuracy: 0.9438\n",
      "Epoch 102/120\n",
      "3337/3337 [==============================] - 2s 486us/sample - loss: 2.2889 - accuracy: 0.9311 - val_loss: 2.4451 - val_accuracy: 0.9438\n",
      "Epoch 103/120\n",
      "3337/3337 [==============================] - 1s 429us/sample - loss: 2.2980 - accuracy: 0.9308 - val_loss: 2.2033 - val_accuracy: 0.9438\n",
      "Epoch 104/120\n",
      "3337/3337 [==============================] - 2s 500us/sample - loss: 2.2792 - accuracy: 0.9311 - val_loss: 2.1799 - val_accuracy: 0.9438\n",
      "Epoch 105/120\n",
      "3337/3337 [==============================] - 1s 440us/sample - loss: 2.3064 - accuracy: 0.9311 - val_loss: 2.2856 - val_accuracy: 0.9438\n",
      "Epoch 106/120\n",
      "3337/3337 [==============================] - 2s 654us/sample - loss: 2.2834 - accuracy: 0.9311 - val_loss: 2.1491 - val_accuracy: 0.9438\n",
      "Epoch 107/120\n",
      "3337/3337 [==============================] - 2s 516us/sample - loss: 2.3035 - accuracy: 0.9311 - val_loss: 2.2648 - val_accuracy: 0.9438\n",
      "Epoch 108/120\n",
      "3337/3337 [==============================] - 2s 636us/sample - loss: 2.2844 - accuracy: 0.9311 - val_loss: 2.2009 - val_accuracy: 0.9438\n",
      "Epoch 109/120\n",
      "3337/3337 [==============================] - 2s 513us/sample - loss: 2.3508 - accuracy: 0.9311 - val_loss: 2.2048 - val_accuracy: 0.9438\n",
      "Epoch 110/120\n",
      "3337/3337 [==============================] - 2s 589us/sample - loss: 2.2564 - accuracy: 0.9311 - val_loss: 2.1476 - val_accuracy: 0.9438\n",
      "Epoch 111/120\n",
      "3337/3337 [==============================] - 2s 455us/sample - loss: 2.2339 - accuracy: 0.9311 - val_loss: 2.1893 - val_accuracy: 0.9438\n",
      "Epoch 112/120\n",
      "3337/3337 [==============================] - 2s 483us/sample - loss: 2.2514 - accuracy: 0.9308 - val_loss: 2.1813 - val_accuracy: 0.9438\n",
      "Epoch 113/120\n",
      "3337/3337 [==============================] - 2s 525us/sample - loss: 2.2616 - accuracy: 0.9311 - val_loss: 2.3094 - val_accuracy: 0.9438\n",
      "Epoch 114/120\n",
      "3337/3337 [==============================] - 2s 455us/sample - loss: 2.3908 - accuracy: 0.9311 - val_loss: 2.2099 - val_accuracy: 0.9398\n",
      "Epoch 115/120\n",
      "3337/3337 [==============================] - 2s 458us/sample - loss: 2.3025 - accuracy: 0.9305 - val_loss: 2.2159 - val_accuracy: 0.9438\n",
      "Epoch 116/120\n",
      "3337/3337 [==============================] - 2s 457us/sample - loss: 2.3560 - accuracy: 0.9305 - val_loss: 2.2473 - val_accuracy: 0.9438\n",
      "Epoch 117/120\n",
      "3337/3337 [==============================] - 2s 528us/sample - loss: 2.4216 - accuracy: 0.9311 - val_loss: 2.3835 - val_accuracy: 0.9438\n",
      "Epoch 118/120\n",
      "3337/3337 [==============================] - 1s 419us/sample - loss: 2.4897 - accuracy: 0.9311 - val_loss: 2.6755 - val_accuracy: 0.9438\n",
      "Epoch 119/120\n",
      "3337/3337 [==============================] - 1s 400us/sample - loss: 2.5478 - accuracy: 0.9293 - val_loss: 2.1757 - val_accuracy: 0.9438\n",
      "Epoch 120/120\n",
      "3337/3337 [==============================] - 1s 445us/sample - loss: 2.2938 - accuracy: 0.9299 - val_loss: 2.1745 - val_accuracy: 0.9438\n"
     ]
    }
   ],
   "source": [
    "auto_model = autoencoder()\n",
    "historty = auto_model.fit(x_train.values,x_train.values,epochs=120,validation_data=(x_test.values[:249], x_test.values[:249]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3337, 513)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1431/1431 [==============================] - 0s 126us/sample - loss: 2.2479 - accuracy: 0.9364\n"
     ]
    }
   ],
   "source": [
    "results = auto_model.evaluate(x_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = auto_model.predict(x_test[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "input_s= Input(shape=(513,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 513)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 400)               205600    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 400)               120400    \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 513)               205713    \n",
      "=================================================================\n",
      "Total params: 772,513\n",
      "Trainable params: 772,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 513)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 400)               205600    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 200)               60200     \n",
      "=================================================================\n",
      "Total params: 386,100\n",
      "Trainable params: 386,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoded = Dense(units=400, activation='relu')(input_s)\n",
    "encoded = Dense(units=300, activation='relu')(encoded)\n",
    "encoded = Dense(units=200, activation='relu')(encoded)\n",
    "# encoded = Dense(units=100, activation='relu')(encoded)\n",
    "# decoded = Dense(units=200, activation='relu')(encoded)\n",
    "decoded = Dense(units=300, activation='relu')(encoded)\n",
    "decoded = Dense(units=400, activation='relu')(decoded)\n",
    "decoded = Dense(units=513, activation='relu')(decoded)\n",
    "autoencoder=Model(input_s, decoded)\n",
    "encoder = Model(input_s, encoded)\n",
    "autoencoder.summary()\n",
    "encoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredLogarithmicError(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3337 samples, validate on 1431 samples\n",
      "Epoch 1/100\n",
      "3337/3337 [==============================] - 3s 785us/step - loss: 5.8336 - accuracy: 0.0069 - val_loss: 5.0868 - val_accuracy: 0.0636\n",
      "Epoch 2/100\n",
      "3337/3337 [==============================] - 2s 632us/step - loss: 4.9645 - accuracy: 0.5901 - val_loss: 4.8080 - val_accuracy: 0.9364\n",
      "Epoch 3/100\n",
      "3337/3337 [==============================] - 2s 670us/step - loss: 4.8089 - accuracy: 0.9299 - val_loss: 4.6208 - val_accuracy: 0.9364\n",
      "Epoch 4/100\n",
      "3337/3337 [==============================] - 2s 610us/step - loss: 4.3974 - accuracy: 0.9311 - val_loss: 4.1126 - val_accuracy: 0.9357\n",
      "Epoch 5/100\n",
      "3337/3337 [==============================] - 2s 621us/step - loss: 4.0035 - accuracy: 0.9305 - val_loss: 3.8394 - val_accuracy: 0.9364\n",
      "Epoch 6/100\n",
      "3337/3337 [==============================] - 2s 624us/step - loss: 3.7806 - accuracy: 0.9308 - val_loss: 3.7367 - val_accuracy: 0.9294\n",
      "Epoch 7/100\n",
      "3337/3337 [==============================] - 2s 737us/step - loss: 3.6974 - accuracy: 0.9281 - val_loss: 3.5393 - val_accuracy: 0.9364\n",
      "Epoch 8/100\n",
      "3337/3337 [==============================] - 2s 718us/step - loss: 3.5964 - accuracy: 0.9281 - val_loss: 3.3926 - val_accuracy: 0.9364\n",
      "Epoch 9/100\n",
      "3337/3337 [==============================] - 2s 637us/step - loss: 3.7781 - accuracy: 0.9296 - val_loss: 3.8383 - val_accuracy: 0.9294\n",
      "Epoch 10/100\n",
      "3337/3337 [==============================] - 2s 641us/step - loss: 3.4590 - accuracy: 0.9248 - val_loss: 3.3675 - val_accuracy: 0.9294\n",
      "Epoch 11/100\n",
      "3337/3337 [==============================] - 2s 710us/step - loss: 3.1626 - accuracy: 0.9248 - val_loss: 3.0643 - val_accuracy: 0.9287\n",
      "Epoch 12/100\n",
      "3337/3337 [==============================] - 2s 737us/step - loss: 3.0404 - accuracy: 0.9260 - val_loss: 2.9526 - val_accuracy: 0.9301\n",
      "Epoch 13/100\n",
      "3337/3337 [==============================] - 3s 933us/step - loss: 2.9804 - accuracy: 0.9272 - val_loss: 2.8929 - val_accuracy: 0.9301\n",
      "Epoch 14/100\n",
      "3337/3337 [==============================] - 2s 653us/step - loss: 2.9459 - accuracy: 0.9254 - val_loss: 2.7943 - val_accuracy: 0.9294\n",
      "Epoch 15/100\n",
      "3337/3337 [==============================] - 2s 638us/step - loss: 2.9558 - accuracy: 0.9275 - val_loss: 3.3232 - val_accuracy: 0.9364\n",
      "Epoch 16/100\n",
      "3337/3337 [==============================] - 2s 652us/step - loss: 3.2587 - accuracy: 0.9311 - val_loss: 3.2685 - val_accuracy: 0.9364\n",
      "Epoch 17/100\n",
      "3337/3337 [==============================] - 2s 636us/step - loss: 3.1695 - accuracy: 0.9278 - val_loss: 2.7316 - val_accuracy: 0.9301\n",
      "Epoch 18/100\n",
      "3337/3337 [==============================] - 2s 643us/step - loss: 2.8982 - accuracy: 0.9254 - val_loss: 2.8823 - val_accuracy: 0.9287\n",
      "Epoch 19/100\n",
      "3337/3337 [==============================] - 2s 692us/step - loss: 2.8353 - accuracy: 0.9260 - val_loss: 2.8616 - val_accuracy: 0.9315\n",
      "Epoch 20/100\n",
      "3337/3337 [==============================] - 2s 737us/step - loss: 2.8888 - accuracy: 0.9287 - val_loss: 2.7386 - val_accuracy: 0.9294\n",
      "Epoch 21/100\n",
      "3337/3337 [==============================] - 3s 825us/step - loss: 2.7177 - accuracy: 0.9254 - val_loss: 2.7384 - val_accuracy: 0.9287\n",
      "Epoch 22/100\n",
      "3337/3337 [==============================] - 3s 948us/step - loss: 2.7502 - accuracy: 0.9287 - val_loss: 2.8192 - val_accuracy: 0.9343\n",
      "Epoch 23/100\n",
      "3337/3337 [==============================] - 2s 687us/step - loss: 2.7975 - accuracy: 0.9281 - val_loss: 2.8314 - val_accuracy: 0.9357\n",
      "Epoch 24/100\n",
      "3337/3337 [==============================] - 2s 634us/step - loss: 2.6938 - accuracy: 0.9272 - val_loss: 2.6107 - val_accuracy: 0.9287\n",
      "Epoch 25/100\n",
      "3337/3337 [==============================] - 2s 677us/step - loss: 2.7287 - accuracy: 0.9272 - val_loss: 2.6372 - val_accuracy: 0.9343\n",
      "Epoch 26/100\n",
      "3337/3337 [==============================] - 3s 758us/step - loss: 2.7085 - accuracy: 0.9257 - val_loss: 2.6379 - val_accuracy: 0.9322\n",
      "Epoch 27/100\n",
      "3337/3337 [==============================] - 3s 774us/step - loss: 2.6603 - accuracy: 0.9257 - val_loss: 2.6453 - val_accuracy: 0.9308\n",
      "Epoch 28/100\n",
      "3337/3337 [==============================] - 3s 759us/step - loss: 2.7894 - accuracy: 0.9251 - val_loss: 2.7386 - val_accuracy: 0.9294\n",
      "Epoch 29/100\n",
      "3337/3337 [==============================] - 3s 768us/step - loss: 2.8763 - accuracy: 0.9266 - val_loss: 2.7584 - val_accuracy: 0.9343\n",
      "Epoch 30/100\n",
      "3337/3337 [==============================] - 3s 762us/step - loss: 3.0279 - accuracy: 0.9302 - val_loss: 3.1094 - val_accuracy: 0.9301\n",
      "Epoch 31/100\n",
      "3337/3337 [==============================] - 3s 888us/step - loss: 3.1200 - accuracy: 0.9251 - val_loss: 3.1045 - val_accuracy: 0.9287\n",
      "Epoch 32/100\n",
      "3337/3337 [==============================] - 3s 773us/step - loss: 3.0412 - accuracy: 0.9254 - val_loss: 3.0032 - val_accuracy: 0.9315\n",
      "Epoch 33/100\n",
      "3337/3337 [==============================] - 2s 746us/step - loss: 2.7106 - accuracy: 0.9269 - val_loss: 2.5722 - val_accuracy: 0.9308\n",
      "Epoch 34/100\n",
      "3337/3337 [==============================] - 2s 740us/step - loss: 2.6839 - accuracy: 0.9257 - val_loss: 2.6932 - val_accuracy: 0.9308\n",
      "Epoch 35/100\n",
      "3337/3337 [==============================] - 3s 946us/step - loss: 2.6354 - accuracy: 0.9272 - val_loss: 2.5858 - val_accuracy: 0.9329\n",
      "Epoch 36/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.9895 - accuracy: 0.9278 - val_loss: 3.2534 - val_accuracy: 0.9364\n",
      "Epoch 37/100\n",
      "3337/3337 [==============================] - 3s 775us/step - loss: 3.3752 - accuracy: 0.9278 - val_loss: 3.0557 - val_accuracy: 0.9357\n",
      "Epoch 38/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.7847 - accuracy: 0.9278 - val_loss: 2.6815 - val_accuracy: 0.9294\n",
      "Epoch 39/100\n",
      "3337/3337 [==============================] - 3s 883us/step - loss: 2.6905 - accuracy: 0.9275 - val_loss: 2.6366 - val_accuracy: 0.9287\n",
      "Epoch 40/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.7244 - accuracy: 0.9266 - val_loss: 2.8712 - val_accuracy: 0.9364\n",
      "Epoch 41/100\n",
      "3337/3337 [==============================] - 3s 1ms/step - loss: 2.6732 - accuracy: 0.9308 - val_loss: 2.5862 - val_accuracy: 0.9357\n",
      "Epoch 42/100\n",
      "3337/3337 [==============================] - 3s 757us/step - loss: 2.5913 - accuracy: 0.9308 - val_loss: 2.5519 - val_accuracy: 0.9343\n",
      "Epoch 43/100\n",
      "3337/3337 [==============================] - 2s 693us/step - loss: 2.7490 - accuracy: 0.9278 - val_loss: 2.7593 - val_accuracy: 0.9287\n",
      "Epoch 44/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.6627 - accuracy: 0.9263 - val_loss: 2.5927 - val_accuracy: 0.9301\n",
      "Epoch 45/100\n",
      "3337/3337 [==============================] - 3s 919us/step - loss: 2.8363 - accuracy: 0.9257 - val_loss: 3.0087 - val_accuracy: 0.9287\n",
      "Epoch 46/100\n",
      "3337/3337 [==============================] - 3s 895us/step - loss: 3.0012 - accuracy: 0.9263 - val_loss: 3.1131 - val_accuracy: 0.9287\n",
      "Epoch 47/100\n",
      "3337/3337 [==============================] - 3s 814us/step - loss: 2.9290 - accuracy: 0.9251 - val_loss: 2.7121 - val_accuracy: 0.9287\n",
      "Epoch 48/100\n",
      "3337/3337 [==============================] - 3s 836us/step - loss: 2.7474 - accuracy: 0.9254 - val_loss: 2.8559 - val_accuracy: 0.9336\n",
      "Epoch 49/100\n",
      "3337/3337 [==============================] - 2s 627us/step - loss: 2.7065 - accuracy: 0.9254 - val_loss: 2.6409 - val_accuracy: 0.9287\n",
      "Epoch 50/100\n",
      "3337/3337 [==============================] - 5s 1ms/step - loss: 2.6725 - accuracy: 0.9251 - val_loss: 2.7112 - val_accuracy: 0.9287\n",
      "Epoch 51/100\n",
      "3337/3337 [==============================] - 3s 935us/step - loss: 2.6621 - accuracy: 0.9248 - val_loss: 2.5971 - val_accuracy: 0.9287\n",
      "Epoch 52/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.6062 - accuracy: 0.9251 - val_loss: 2.6032 - val_accuracy: 0.9287\n",
      "Epoch 53/100\n",
      "3337/3337 [==============================] - 2s 739us/step - loss: 2.5603 - accuracy: 0.9248 - val_loss: 2.5256 - val_accuracy: 0.9287\n",
      "Epoch 54/100\n",
      "3337/3337 [==============================] - 2s 716us/step - loss: 2.6662 - accuracy: 0.9254 - val_loss: 2.6166 - val_accuracy: 0.9287\n",
      "Epoch 55/100\n",
      "3337/3337 [==============================] - 2s 744us/step - loss: 2.7377 - accuracy: 0.9266 - val_loss: 2.7770 - val_accuracy: 0.9308\n",
      "Epoch 56/100\n",
      "3337/3337 [==============================] - 2s 698us/step - loss: 2.6563 - accuracy: 0.9263 - val_loss: 2.6766 - val_accuracy: 0.9287\n",
      "Epoch 57/100\n",
      "3337/3337 [==============================] - 2s 691us/step - loss: 2.6385 - accuracy: 0.9254 - val_loss: 2.5752 - val_accuracy: 0.9287\n",
      "Epoch 58/100\n",
      "3337/3337 [==============================] - 3s 945us/step - loss: 2.5881 - accuracy: 0.9260 - val_loss: 2.8524 - val_accuracy: 0.9336\n",
      "Epoch 59/100\n",
      "3337/3337 [==============================] - 3s 862us/step - loss: 2.6673 - accuracy: 0.9287 - val_loss: 2.5379 - val_accuracy: 0.9315\n",
      "Epoch 60/100\n",
      "3337/3337 [==============================] - 2s 691us/step - loss: 2.5952 - accuracy: 0.9269 - val_loss: 2.6545 - val_accuracy: 0.9322\n",
      "Epoch 61/100\n",
      "3337/3337 [==============================] - 3s 852us/step - loss: 2.6841 - accuracy: 0.9275 - val_loss: 2.6032 - val_accuracy: 0.9301\n",
      "Epoch 62/100\n",
      "3337/3337 [==============================] - 3s 958us/step - loss: 2.6232 - accuracy: 0.9248 - val_loss: 2.6080 - val_accuracy: 0.9294\n",
      "Epoch 63/100\n",
      "3337/3337 [==============================] - 3s 816us/step - loss: 2.5921 - accuracy: 0.9248 - val_loss: 2.6555 - val_accuracy: 0.9294\n",
      "Epoch 64/100\n",
      "3337/3337 [==============================] - 3s 784us/step - loss: 2.7216 - accuracy: 0.9251 - val_loss: 3.3930 - val_accuracy: 0.9287\n",
      "Epoch 65/100\n",
      "3337/3337 [==============================] - 3s 789us/step - loss: 2.6681 - accuracy: 0.9254 - val_loss: 2.5630 - val_accuracy: 0.9294\n",
      "Epoch 66/100\n",
      "3337/3337 [==============================] - 3s 821us/step - loss: 2.5590 - accuracy: 0.9257 - val_loss: 2.6533 - val_accuracy: 0.9280\n",
      "Epoch 67/100\n",
      "3337/3337 [==============================] - 3s 813us/step - loss: 2.8642 - accuracy: 0.9260 - val_loss: 2.7750 - val_accuracy: 0.9308\n",
      "Epoch 68/100\n",
      "3337/3337 [==============================] - 3s 861us/step - loss: 2.6931 - accuracy: 0.9146 - val_loss: 2.5928 - val_accuracy: 0.9057 loss: 2.7685 - accu - ETA\n",
      "Epoch 69/100\n",
      "3337/3337 [==============================] - 3s 793us/step - loss: 2.6593 - accuracy: 0.9146 - val_loss: 2.5936 - val_accuracy: 0.9064\n",
      "Epoch 70/100\n",
      "3337/3337 [==============================] - 3s 807us/step - loss: 2.6642 - accuracy: 0.9140 - val_loss: 2.6449 - val_accuracy: 0.9308\n",
      "Epoch 71/100\n",
      "3337/3337 [==============================] - 3s 832us/step - loss: 2.6174 - accuracy: 0.9194 - val_loss: 2.5439 - val_accuracy: 0.9126\n",
      "Epoch 72/100\n",
      "3337/3337 [==============================] - 3s 937us/step - loss: 2.5739 - accuracy: 0.9056 - val_loss: 2.5545 - val_accuracy: 0.8910\n",
      "Epoch 73/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.7492 - accuracy: 0.9176 - val_loss: 2.7154 - val_accuracy: 0.9287\n",
      "Epoch 74/100\n",
      "3337/3337 [==============================] - 3s 993us/step - loss: 2.7700 - accuracy: 0.9224 - val_loss: 3.3045 - val_accuracy: 0.9252\n",
      "Epoch 75/100\n",
      "3337/3337 [==============================] - 3s 1ms/step - loss: 3.0822 - accuracy: 0.9224 - val_loss: 2.7864 - val_accuracy: 0.9238\n",
      "Epoch 76/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.9923 - accuracy: 0.9158 - val_loss: 3.0577 - val_accuracy: 0.9140\n",
      "Epoch 77/100\n",
      "3337/3337 [==============================] - 3s 998us/step - loss: 2.9811 - accuracy: 0.9119 - val_loss: 2.5268 - val_accuracy: 0.9238\n",
      "Epoch 78/100\n",
      "3337/3337 [==============================] - 3s 912us/step - loss: 2.5596 - accuracy: 0.9068 - val_loss: 2.6381 - val_accuracy: 0.8973\n",
      "Epoch 79/100\n",
      "3337/3337 [==============================] - 3s 981us/step - loss: 2.6701 - accuracy: 0.8981 - val_loss: 2.4859 - val_accuracy: 0.8896\n",
      "Epoch 80/100\n",
      "3337/3337 [==============================] - 3s 950us/step - loss: 2.5239 - accuracy: 0.8885 - val_loss: 2.4953 - val_accuracy: 0.8854\n",
      "Epoch 81/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.5904 - accuracy: 0.9008 - val_loss: 2.6582 - val_accuracy: 0.9287\n",
      "Epoch 82/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.8298 - accuracy: 0.9245 - val_loss: 2.6964 - val_accuracy: 0.9301\n",
      "Epoch 83/100\n",
      "3337/3337 [==============================] - 3s 999us/step - loss: 2.6605 - accuracy: 0.9230 - val_loss: 2.5944 - val_accuracy: 0.9238\n",
      "Epoch 84/100\n",
      "3337/3337 [==============================] - 3s 787us/step - loss: 2.5599 - accuracy: 0.8735 - val_loss: 2.4959 - val_accuracy: 0.8882\n",
      "Epoch 85/100\n",
      "3337/3337 [==============================] - 3s 767us/step - loss: 2.5578 - accuracy: 0.8376 - val_loss: 2.5184 - val_accuracy: 0.8001\n",
      "Epoch 86/100\n",
      "3337/3337 [==============================] - 2s 742us/step - loss: 2.5573 - accuracy: 0.7621 - val_loss: 2.5648 - val_accuracy: 0.7142\n",
      "Epoch 87/100\n",
      "3337/3337 [==============================] - 3s 778us/step - loss: 2.5139 - accuracy: 0.7009 - val_loss: 2.4751 - val_accuracy: 0.6876\n",
      "Epoch 88/100\n",
      "3337/3337 [==============================] - 3s 777us/step - loss: 2.4824 - accuracy: 0.7723 - val_loss: 2.4613 - val_accuracy: 0.8854\n",
      "Epoch 89/100\n",
      "3337/3337 [==============================] - 3s 842us/step - loss: 2.4576 - accuracy: 0.8687 - val_loss: 2.5148 - val_accuracy: 0.9036\n",
      "Epoch 90/100\n",
      "3337/3337 [==============================] - 3s 893us/step - loss: 2.4916 - accuracy: 0.8969 - val_loss: 2.5594 - val_accuracy: 0.9078\n",
      "Epoch 91/100\n",
      "3337/3337 [==============================] - 3s 928us/step - loss: 2.4878 - accuracy: 0.8885 - val_loss: 2.4457 - val_accuracy: 0.8735\n",
      "Epoch 92/100\n",
      "3337/3337 [==============================] - 3s 949us/step - loss: 2.5286 - accuracy: 0.8807 - val_loss: 2.4408 - val_accuracy: 0.8847\n",
      "Epoch 93/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.5435 - accuracy: 0.8693 - val_loss: 2.5184 - val_accuracy: 0.9113\n",
      "Epoch 94/100\n",
      "3337/3337 [==============================] - 4s 1ms/step - loss: 2.6141 - accuracy: 0.8843 - val_loss: 2.5156 - val_accuracy: 0.8120\n",
      "Epoch 95/100\n",
      "3337/3337 [==============================] - 3s 998us/step - loss: 2.5092 - accuracy: 0.8475 - val_loss: 2.4560 - val_accuracy: 0.8316\n",
      "Epoch 96/100\n",
      "3337/3337 [==============================] - 3s 824us/step - loss: 2.4986 - accuracy: 0.8241 - val_loss: 2.4624 - val_accuracy: 0.8393\n",
      "Epoch 97/100\n",
      "3337/3337 [==============================] - 3s 812us/step - loss: 2.6904 - accuracy: 0.9119 - val_loss: 2.9704 - val_accuracy: 0.9287\n",
      "Epoch 98/100\n",
      "3337/3337 [==============================] - 3s 972us/step - loss: 2.6971 - accuracy: 0.9215 - val_loss: 2.6072 - val_accuracy: 0.9113\n",
      "Epoch 99/100\n",
      "3337/3337 [==============================] - 3s 877us/step - loss: 2.5525 - accuracy: 0.9080 - val_loss: 2.6415 - val_accuracy: 0.8791\n",
      "Epoch 100/100\n",
      "3337/3337 [==============================] - 3s 882us/step - loss: 2.6949 - accuracy: 0.9065 - val_loss: 2.7297 - val_accuracy: 0.9315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15931d950>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,                \n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = encoder.predict(x_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.    ,    0.    ,    0.    , ...,    0.    , 2763.7778,\n",
       "           0.    ],\n",
       "       [   0.    ,    0.    ,    0.    , ...,    0.    , 3746.355 ,\n",
       "           0.    ],\n",
       "       [   0.    ,    0.    ,    0.    , ...,    0.    , 3891.8914,\n",
       "           0.    ],\n",
       "       ...,\n",
       "       [   0.    ,    0.    ,    0.    , ...,    0.    , 3039.4053,\n",
       "           0.    ],\n",
       "       [   0.    ,    0.    ,    0.    , ...,    0.    , 3563.9736,\n",
       "           0.    ],\n",
       "       [   0.    ,    0.    ,    0.    , ...,    0.    , 3323.5854,\n",
       "           0.    ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm.fit(new_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train[0:100], model_svm.predict(new_features[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
